Contents lists available at  journal  homepage:  
A higher-order Markov chain-modulated model for electricity spot-price dynamics c Division of Physical Sciences and Mathematics, University of the Philippines Visayas, Miag-ao, Iloilo,  Philippines
• Mean reversion, seasonality, memory, spike, and stochasticity are altogether captured.
• Model implementation and validation were performed using observed data.
Keywords:
 A B S T R A C T 
Over the last three decades, the electricity  sector worldwide underwent  massive deregulation.  Power market participants  have encountered a growing number of challenges due to competition  and other pertinent factors. For most markets at present, electricity  is a non-storable commodity  and its price is extremely  sensitive to changes in supply and demand. The evolution  of electricity  prices exhibits  pronounced mean reversion and cyclical patterns, possesses extreme volatility and relatively  frequently occurring spikes, and manifests presence of memory property.  These observed features necessitate the development of models aimed to simultaneously capture such price characteristics for forecasting, risk management, and valuation  of electricity-driven derivatives. Majority of current research studies examining electricity  spot prices are based on one-state models. Such models though may not accurately capture the stylised behaviours of price evolution,  especially during periods of sudden spikes driven  by the abrupt  changes of market  sentiments. This paper tackles the modelling  and estimation  problems under a new paradigm that integrates the deterministic calendar seasons and stochastic factors governing  electricity  prices with  a regime-switching  feature. The de-seasonalised component  of our proposed model has both  the jump  and mean-reverting  properties to account for  spikes and periodic  cycles alternating between lower price returns and compensating periods of higher price returns. The parameters of the de-seasonalised model components are also modulated  by a higher-order  hidden Markov  chain (HOHMC) in discrete time. This provides a mechanism to extract latent information from historical  data. The HOHMC’s state is interpreted as the “state of the world” resulting from the interaction  of various forces impacting the electricity market. Filters are developed to generate optimal estimates of HOHMC-relevant quantities using the observation process, and these provide  online  estimates of model parameters. Empirical  demonstrations  using the daily electricity  spot prices, compiled by the Alberta Electric System Operator (AESO), show that our HOHMM approach has considerable merits in terms of price data fitting and forecasting metrics. Our proposed framework driven by a hidden Markov chain with  a state memory-capturing  mechanism provides a powerful  alternative to support the pricing  of contracts in the electricity  markets.
 z	filtration generated by {Wk}
J	filtration generated by {Jk}
A 	RN ×N
 
 
  tsr k
  process of number of jumps of yw
 
 diagonal probability  matrix
 state et
 occupation time process that yw spent in state (e , e )
 k
C t (f )
 k 	t      s
P	real-world probability  measure
 Dt	deterministic component of the price evolutions
D w [ · ] 	filtering  recursions for the quantity of interest
 transition probability  matrix
 St	electricity spot price speed of mean reversion of the deseasonalised process mean-reverting level of the deseasonalised process volatility of the deseasonalised process
 Uk 	Xk -measurable process yw	second-order hidden Markov chain w
P t 	Poisson counter µ	mean of the jump size
2	variance of the jump size
 k 	Radon-Nikodym derivative density function of a standard normal random variable transformation for the second-order HMC yw w      w
E w	set of HOHMM-based parameters
 gk	conditional expectation
 [   (yk , yk  1) |Xk]
,   ,  ,	parameters of the stochastic component
 k 	conditional expectation   P	k
 (yk , yk  1) |Xk]
{m w}
 sequence of IID standard normal random variables. sequence of martingale increments
 Pt
 Poisson process
(  , F , P) underlying probability  space w	filtration generated by {yw}
 (t, T ) 	price of the risk process
G (t, T ) 	expected value of the spot price for delivery at T
1.  Introduction
 pricing  models that mimick  model-pricing developments in the commodity market. However, due to the unique characteristics of electricity spot prices, many financial  models designed for regular commodities cannot be adapted necessarily to the electricity markets.
 switching  approach. Huisman and Mahieu [17]  proposed a regimeshifting jump process with  a recovery state and concluded that it performed better than a stochastic jump model whilst  a two-regimeswitching  model with  ‘abnormal’ and ‘normal’  states was shown superior to a Poisson-jump model in De Jong [18] in capturing electricityprice dynamics. Wu et al. [19]  studied the Alberta electricity-spot market via a hidden Markov model with  a multi-model  identification approach. Alberta electricity  pool prices are divided into five classes, and the last three classes with prices over $100/MWh are deemed highpool-price regions. It was found that price-forecasting performance could be improved substantially, especially in high-pool-price regions, by incorporating a Markovian regime-switching mechanism.
In  the  context  of  regime-switching  approaches, hidden  Markov models (HMMs)  have been widely  successful in  many engineering, economic and financial  applications; see Mamon and Elliott  [20,21]. HMMs  are  beneficial  for  modelling  processes illustrating   regimeswitching dynamics via Markov chains with latent states. This approach was previously  applied in  the examination of electricity-price  behaviour.  A non-stationary model based on input-output  HMM  was developed to model time series of spot prices in the Spanish electricity market [22].  As well, Yu and Sheble [23]  utilised HMMs to efficiently model price movements in  the electricity  market and provide  good forecasts adjudged from the perspective of accuracy and dynamic information in the US market.
 annual, semi-annual, quarterly,  monthly,  weekly, and mid-week patterns. The constants a, b, ch, eh, gh, and jh are recovered from a data set
In the ensuing exposition, we shall denote all vectors and matrices by bold small English/Greek letters and bold capitalised English/Greek letters, respectively. The deseasonalised component Xt in (2) follows an OU model and an additive compound-Poisson component; these equip the capability to model mean reversion, price variations and spike dynamics. The process Xt  in Eq. (1) satisfies the stochastic differential equation (SDE)
 dXt =    (
 Xt ) dt +
 dWt + dJt ,
 (3)
 where  ,   , and   stand for the speed of mean reversion, mean-reverting via the EM algorithm in Section 4. Numerical implementation, which
 level,  and volatility, respectively. A  Brownian  motion  {Wt }
 models
 the random price fluctuations under a stable market condition, whilst
{Jt } is designed to pick up the price spikes. A compound Poisson process affords capacity to model jumps, as adduced in [35,36]  and similar
 to  their  settings, we let  dJt  =
 dPt ,  where Pt
 is a Poisson process with  a  constant intensity 	and a  normally  distributed  jump  size
2.  Model formulation
 N (µ ,
 2). For s	t , the continuous-time solution of Eq. (3), by
 Itô’s lemma, is
Xt = Xs e  (t   s)  + (1  e
 
(t   s) )
 
+   e    t
  t 	t e  udWu + 	e
 
(t   u) dJu.
 
(4)
 By approximating the distributions of the stochastic integrals in (4),
P tk   1
 Xk +1 = Xk e
 tk+   + (1  e
 tk+  )   +
 1  e  2
 tk+ 1
 zk +1 +
 + e     (  tk+ 	)     ,
1	1 folding of the log-spot prices in the Nordic exchange. Yousef [32]  applied mean-reverting diffusion models to the Alberta electricity market;
 m=1
 1    m    m
 where
 tk+1 = tk+1
 tk  for k	: =   +
 {0},  m  is the occurrence time electricity  spot prices. Invoking  Benth et al.  [15],  we suppose the
(  , F , P ), is given by
 of the mth  jump, and {zk+1} is a sequence of independent and identically
 (1)
 begin with  a homogeneous Markov chain yk  with  a finite  state space
{e1, e2, …, eN }, where ei  = (0, …, 0, 1, 0, …, 0)
 N  with 1 in the ith po-
 sition,     is the matrix transpose operator, and N is the state-space dimension. Model parameters in Eq. (5) switch randomly, in accordance with the dictates of yk , amongst different electricity-market regimes as time progresses. With the canonical basis as yk ’s state space, we have
 k 	(yk) =
 , yk  ,  k
 (yk) =
 , yk  ,  k
 (yk) =
 , yk  ,   and   ,
Trigonometric functions were applied by various authors to capture
 where  ·,·  is the inner product in   N . The stochastic process Xk in Eq.
(5) then can be written  as
 Xk+1 = Xk e
 (yk)  tk+1  + (1  e
 (yk)  tk+ 1 )  (yk)
 
+   (yk)
 1  e  2  (yk)  tk+ 1
2  (yk)
 
 P tk+ 1 e
 
(yk) (  tk +1
  m )      (y ).
 
3
 To completely characterise the parameter estimation and filtering under the HOHMM setting, we concentrate on a second-order hidden Markov chain. Of course, in theory and principle, the estimation and
 
 chsin  dh t
 2
 + ehcos  dh t
 2
 2
 2
 ,
 be extended in a straightforward  manner, notwithstanding  the corresponding  computational  challenge.  Suppose yw   is  a  second-order hidden Markov chain regulating the evolution  of   ,   ,  , and   . We w	z	J	w	z where d1 = 1, d2 = 2, and d3 = 4 to cover periodic trends including the
 define Fk
 Fk	Fk
 Fk    as the global filtration, where Fk  , Fk ,
 AppUedEl ;zy 233-234 (2019) 495-515
 N
 nding  optimal filters  is easy  because  the  interchange of
 expectations and  summations is  permissible  and  justified   by  the  Fu. birηleo 	m [39].ηle estimated processes' dynamics under P can be
 (7)	where
 
 (8)
 
 
9(y) = E(y  )
 
  τ(y) = 	L e.....(Y'k)(4tk+1-Vh)f1h (y  )
 .:2
 probability  transition  matrix   with   entries	h 1
 (14)
Deriving recursive filtersunder the  real world  probability P with the  realistic supposition of dependent obser rationscould   be  compu. tationally expensive and cumbersome. To this endwe shall perform  the recursive filtering involving yunder an  ideal  reference probability
 
HOHi\α4
 HMM
 Measure change	HMM
(measure P)
Reversed measure change	Information state filter
 Ideal \Vorld
2     w	1	   	w      	
 w	   	w w	{ (   (yl   1) )   [Xl l
 (yl   1)	(yl   1) Xl   1
( 2 (yw   ) )  (X )
 (yl   1) ] }
 (i) l   1	l w	w	w
 k tsr
  yw   , er
  yw  , es
  yw, et  ,
 (  (yl   1) +
 (yl   1) Xl   1 +
2 (yw   )
 (yl   1) ) Xl
 l   2
 l   1	l
 (20)
(  (yw  ) +
 
(yw   ) Xl   1 +
 
(yw   ) )2
 which refers to the number of jumps from state (er , es) to state et up
2     w	,
 to time k, where 2
 l 	k and r, s, t = 1,
 … , N ;
2   (yl   1) where	is the  probability   density  function  of  a standard normal w
 (ii) k t
  w	t 	w t 	k   1	t
 0 = 1, {
 l, l
 +} is an Fl   -adapted martingale
 Bk  =
 l=2
 yl   1, e   = B
 +   yk  1, e  ,
 (21)
3.2. Calculation of recursive filters
Under P  where the observation process is IID,  calculations and
 that yk  spent in state et , where 2 k
 l 	k and t = 1,
 … , N ;
 ts	w k
 , et
 yw   , es   , which will  be used to estimate the HOHMC yw . The Bayes’ theorem for
 B	l   1	l   2
 (22) conditional expectation is a handy tool in getting optimal estimates of
 which represents to the occupation time up to time k or the length of various quantities under P, using the results from recursive filters under
 time   that   yw    spent  in   state
 (et , es),   where   2
 l 	k   and
In  particular,  let  gk = (gk (1), gk (2), …, gk (cb),  …, gk (NN ) )
 
 s, t = 1,
 … , N ; w	w	w      w where gk (cb) w
 P (yk  = ec , yk  1 = eb|Xk ) = w
 [ 	(yk , yk  1), ecb  |Xk]. A	k t 	w	t 	w
 (yk , yk  1) under P is given by
 Ck (f ) =
 
 f (Xl ) yl   1, et
 = Ck   1 (f ) + f (Xk ) yk  1, et  ,
 (23)
P	w	w      w w      w	[  k
 (yk , yk  1) |Xk] 	w gk: =
 [   (yk , yk  1) |Xk] = 	w	.
 which is an auxiliary process related to yk  for some function f up to
P [  k |Xk]
 (16)
 time k in state et , where 2
 l 	k, t = 1,
 … , N . Here, f takes the w	w      w
 N	w      w
 functional forms f (X ) = X , f (X ) = (X )2 or f (X ) = Xk  1 Xk .
Write
 k 	[  k
 (yk , yk  1) |Xk].  Since
2
 c,b=1
 (yk , y
 k   1), ecb
= 	(yk , yk  1), 1
 = 1, where 1 is an    N
 vector with  1 in all of its
 Consequently, the conditional expectation of
 (yw
 , yw) in Eq. (18) entries, we have
N	N
  w	w      w
 can be written  recursively as
 k+1      k
 
(24)
 k, ecb
 = c, b=1
 P	k      (yk , yk  1) |Xk], ecb
 
Suppose U  is any X -measurable process, denoting any of the quantities k 	k
N	w	w w	w      w
 P	w	in  Eqs. (20)–(23).  Write  Dk  [Uk]
 P [  k Uk |Xk] and Uk
 [Uk |Xk].
= 	k
 (yk , yk  1), ecb   |Xk    = w
 [  k |Xk].
 
 Here, Uk  is regarded as the ‘best estimate’ of Uk . Similar to the steps in establishing Eq. (18), the conditional expectation of Uk given Xk  can be obtained using calculations that are entirely under P by observing that
Therefore, the filter  of
 (yk+1, yk ) in Eq. (16) under P has an explicit
 
P	w	w
 U  =    [U |X ] =
 [  k Uk |Xk] 	Dk  (Uk )
= 	. gk =    N
 
= 	.
 k 	k     k
 P [  k |Xk]
 k, 1
 (25) k, ecb
 Evaluating the numerator  of  (25)  (cf  Mamon et al.  [42])   for  each terms of
 (yk+1, yk ), we need to construct two N  × N
 matrices by
 parameters. This will  be elaborated further in Section 4. following   Xi   and  Mamon  [26].   These  are  Kt     with   eit     on  its
 1) N + t )th column and 0 elsewhere for 1
 i, t 	N , and a diagonal
 Proposition 1. The filtering recursions for the respective quantity in Eqs.
 (20)–(23) are
1	w	tsr	w	w
  w	tsr	w      w hk     0	0
 Dk+1 (Ak+1
 (yk+1, yk )) = AHk+1Dk   (Ak
 (yk , yk  1))
0	0	+
N
 gk , esr   hk+1  Aesr , ets  ets,
 (26)
Hk  =
 0   hk
 
 
 
  w	t	w	w
1	k   1     k
 w	t 	w      w	t
1	k 	k   1	k
0	0
 
 (27) t
0	0   hk
 Dk+1 (Bk+ 1
 (yk+1, yk )) = AHk+1Dk   (Bk+ 1
 (yk , yk  1)) +   gk , ets  hk+ 1 Aets,
 (28) i	{ ( 2) 1 [Xk
 
 
Dk+1 (Ck+1 (f )   (y
 
, y )) = AHk   1Dk  (Ck+ 1 (f )    (y  , y
 
 .
 w	t
 w	w	w      t
 w     w
 t
Following  Xiong and Mamon [25],  we define certain scalar processes of interest involving  the HOHMC yw . These are as follows:
 
  both PE
 w and PE
 
 T = (ptsr )  and T = (ptsr ).  To estimate the  transition  probabilities  in succession, i.e., by substituting T with T, we utilise the likelihood function in combination with the EM algorithm and consider w	w	w
 T 	dPE
 k 	N tsr
 yl   2, er   yl   1,es    yl , et aptly  requires an efficient  iterative  approach. Refer to  [31,25,26], amongst others, for further details of the EM algorithm.
 k  =  dPE0 |Xk  = ptsr
 
 
 ptsr
 ,
 where ptsr
 = 1 for  ptsr  = 0 and ptsr  = 0. The estimation of parameters the probability  density function  (pdf)  of  Xt , which  is needed in the maximisation of the appropriate expected log-likelihood function given the information  reflected in Xt . Model parameters are governed by yw but suppose that the HOHMM and all model parameters remain un-
 given the observation process following  Eq. (10) is accomplished by blending the EM algorithm  and recursive filters in (26)–(29). The resulting outcomes for ptsr  and the rest parameters are given as follows.
 [s, t].   Then,  the  regular
 Proposition 2. The EM estimates, at state t given a series of observations
2	2  (t   s)
 Xk+1 for k	0, are tion with mean Xs e  (t   s)  + (1  e
 (t   s) )
 and variance
 (1    e	) .
 t
C    (X , X	)
 t 	t
C    (X ) + µ  qC    (X	)
 t qµ  B
2
 k      k 	k+1 	t 	k      k 	t t = 	t 	2	t
 k      k+1 	t 	t 	k
2	t of the jump component Jt  is also determined. As stated in Section 2.2, Jt
 C  k (Xk ) + 2µ t qC  k (Xk ) + (µ t q) B  k relies  on	and  P .  We  recall  that 	N (µ ,
 2)  and  P  has a
 Dk  (Ck (Xk , Xk+1) )
 t Dk  (Ck (Xk ) ) + µ  qD
 (Ck (Xk+1 ) ) w	t 	w	t 	w	t
 	t         k constant  jump   intensity 	.   Utilising   the  integral   approximation
 = 	w	t 	2	w	t
 2    w	t t	(t   u)	(t   u)       	
 Dk  (Ck (Xk ) ) + 2µ t qDk  (Ck (Xk ) ) + (µ t q) Dk  (Bk ) s   e	dJu 	e
 (Jt
 Js) in Erlwein et al. [31],  the density of the	w	t
 t qµ  Dk  (Bk )
, w	t	2	w	t
 2    w	t
J       (x ) =
 ( (t  s) )m e
 
(t   s)  ×
 
(x; µ e
 
(t   s) m,
 
2 e  2  (t   s) m).
 Dk  (Ck (Xk ) ) + 2µ t qDk  (Ck (Xk ) ) + (µ t q) Dk  (Bk )
 (33) t   s	m!
 t
C    (X	)
 t
C    (X )
 
 t
(X )
 k      k+1
 t 	k      k 	t     t 	k      k
 B  k w	t 	   	w	t 	   	w	t
Dk  (Ck (Xk+1) )
 t Dk  (Ck (Xk ) )
 t µ  qDk  (Bk ) also the density function of the increment Jt
 Js .
 = 	w	t 	,
 Dk  (Bk )
 (34)
[43], and just focusing first on the non-switching setting, we can utilise the
 t 	2	2	t 	2 convolution of the OU and Jt ’s densities to get the probability density of Xt
 ^2	C  k (Xk+1) + t
 t  C  k (Xk )
 B  k t     2	2	     2    2
 ( (t  s) )m
 
(t   s) 	(t   s) 	          (t   s)
 B k ( t  + ( t µ t q)
 + 2 t   t µ  q
 t    t  q)
Xt |Xs (x ) =
 e m=0 	m!
 × 	x; Xs e
 + (1  e	) 	+ 	t t
+ µ me
 (t   s) ,
 2 (1
 e  2  (t   s) )
2	+
 2 me  2  (t   s)
 C    (X ) (2µ  q
+ 	t
B k
 t   + 2 t   t ) t 	t
=    P t
 x; Xs e  (t   s)  + (1  e
 (t   s) )
 (2 t  + 2 t µ  q) C  k (Xk+1) + 2 t C  k (Xk+1 Xk ) t 	,
 
(35)
+ µ P t e
 (t   s) ,
 2 (1
 e  2  (t   s) )
2	+
 2 P t e  2  (t   s) 	,
 
(31)
  t µ	C  k (Xk+1)
  t
  where P t  denotes a Poisson counter.
 t qB  k (Xk ) w	t 	   	w	t 	   	w	t
Going back to the implementation of the EM algorithm under our
 Dk  (Ck (Xk+1) )
 t Dk  (Bk (Xk ) )
 t Dk  (Ck (Xk ) )
HOHMM setting, let P t  be q and assume (for tractability) that Pt   is independent of other parameters in the model. From (10) and (31), the
 t qDk  (Bk (Xk ) )
 (36) t	2	2	t	2
 t     2	2	    2 discrete-time process Xk , under the HOHMM framework, is set to follow
 2	C  k (Xk+1) +
 t  C  k (Xk )
 B k ( t  + ( t µ t q)
 + 2 t   t µ  q    t )
 t  = 	t    2	+
B k   t
 t
B k   t w	w	w	w	w	t 	2 µX (yk ) =
 (yk ) Xk +
 (yk ) + µ (yk ) q
 (yk ),
 C  k (Xk ) (2µ t q t   + 2 t   t )
 
2	2     w
 + 	t
B k   t  q t 	t
X (yk ) =
 (yk ) +
 (yw) q
 (yk ).
 (2 t  + 2 t µ  q) C  k (Xk+1) + 2 t C  k (Xk+1 Xk )
 	t  	,
  tsr
 t
B k   t  q
 (37)
E w = { t,
 t ,  t , µ
 ,	, ptsr , 1
 t, s, r 	N } as the set of HOHMM-based
 A k 	Dk  (Ak p   = 	=
 )
,	pairs (t, s),
 t 	s. t 	t 	tsr
 sr	w	sr parameters.  Starting  with   the  set	w
 of  initial   parameters,  the w
 B k 	Dk  (Bk  )
 (38) w
 Proof. See Appendix A.   □ w	w	w
 dPE
E 	argmaxE w L (E
 )   and
 L (E   ) =
 E0
 Xk   .	Following
 The calculations involved in (33)–(38) are similar to those in Erl-
 wein et al. [31],  and can be straightforwardly  reproduced. The algorithm  for deriving the EM estimates in terms of the recursive filters is
 and entries are updated automatically  through  the
 described thoroughly in [31],  Xi and Mamon [26],  and Xi et al. [30].
 We apply the EM algorithm  to obtain  the optimal  estimates of the
 Table 1
First, a transformation from the parameter set E w to E
 w
 Descriptive statistics for daily electricity  spot price (DESP). w
 
Mean 	Std Deviation	Std Error 	Median
2	2
 economic operation  of  the  Alberta  Interconnected  Electric  System t,  t ,
 t , µ t ,
 , and ptsr , 1
 t, s, r 	N whenever a new sequence of
 DESP exhibit a long-run mean-reverting level and cyclical patterns. For
The development of these recursive filters thereby implies that the parameter estimates under our HOHMM setting are self-calibrating. The results in Propositions 1 and 2 constitute further research progress relative to those given in Erlwein et al. [31],  and Xiong and Mamon [25] in the following respects. Firstly, self-calibrating filtering algorithms for electricity spot prices in a regular HMM setting [31]  are extended to a general HOHMM case. Secondly, we include a compound Poisson process to depict the spikes in the price dynamics; such inclusion was not a consideration item in  the HOHMM setting of [25].  Thirdly,  Erlwein et al. [31]  went though the route of estimating first the entire drift component before being able to compute an estimate for the mean-reverting  level   . In our case, we directly  dealt with  the MLE of     by providing an explicit solution as a function of filtering  recursions. The required sequence of computations in recovering the model parameters is clarified.
 instance, there are more extreme values and changes in temperature
Electricity spot prices are gathered with daily frequency and so we
 assign
 t = 1. We process the observations in 73 batches with 20 data
Alberta launched the first wholesale electricity market in Canada. It prescribes that if the wholesale electrical energy generated in the province is not consumed on site, it must flow  through the Power Pool operated by the AESO [45].  The AESO, on behalf of Albertans, runs a fair and openly competitive electricity market and offers a reliable and
 points in each batch. In this sense, the parameters are updated roughly every  3 weeks. Other  filtering   window  sizes can be explored  too; however, within  the data set that  we analyse, our experimentation produces similar outcomes, telling us that with different window sizes have negligible  effect. We note that  a 20-point  data length  for  the HOHMM filtering  procedure is fairly  sufficient  and not numerically onerous in processing the continual flow of new information  including those resulting  from  abrupt  changes in  the DESP dynamics due to
Fig. 2. Fitted seasonal component and observed spot prices.
Parameter estimates for the seasonal component Dt .
 = 0.6394,
 =    0.1515, ^ = 0.6443, µ  =
 0.4945, and	= 0.0103.
Parameter 	Estimate 	95% confidence  interval a                                             0.0181                                    (  0.0290,   0.0071) b                                           80.7214                                         (71.5387, 89.9041) c1                                                                 6.4621                                    (  12.9686, 0.0445) c2                                                               8.9067                                           (2.4942, 15.3191) g2                                                                  7.6311                                   (  14.0103,   1.2519) e1                                                                     6.2641                                    (  12.6405, 0.1123) j1                                                      20.1426                                 (  26.5187,   13.7664)
0 < d < 0.5, there is a finite  long memory in the data; and d = 0 signifies short memory. In computing d, one may employ the GewekePorter-Hudak estimator, approximated MLE, and smoothed periodogram approach; estimated values for d could be easily returned by utilising the R functions ‘fdGPH’, ‘fdML’, and ‘fdSperio’, respectively. In our case, we employ ‘fdSperio’, proposed by Reisen [48],  since unlike the two former algorithms, the latter algorithm has no restriction and is applicable to a non-stationary  process. Our de-seasonalised data set gives d = 0.20. w = { ,   ,  , µ ,    , p} ; for emphasis, these parameters are driven by a discrete-time finite-state  HOHMC. This goal is accomplished by initialising   estimates and  updating  subsequently the  parameter  set.
 
Benchmarks for starting values are attained by treating Xt  as a singleregime process. This implies that the transition probability  matrix T is identity.  From (10), the likelihood  function of Xk+1 is
 
 w) =        E w
 2
        logL (X ; E
  w)  .
 
 
 This provides a bound on the asymptotic variance of the MLEs. The ML
L (Xk+1;  ,   ,  , µ ,    ) =
 2( 2 +   2 q 2)
  distribution;  see [49].  We utilise the limiting distribution  of the MLE,
 2  ( 2 +
 2 q 2)
 (39)
 w
E 	, to get the 95% confidence interval. For a generic (scalar) estimate where 1	m
 
 w , this is ^w  ± 1.96
 1
 . Derivation  of I ( w ) makes use of the log- minimising the negative of the log likelihood,  i.e.
 likelihood  functions, and this is delegated in the Appendix A. The re- m
  log   2  ( 2 +
 
 (Xk+1
 Xk
 µ q )2
 
 
I (p
  tsr
) = 	,
  t
B k
I (  ) = 	,
 
I (µ  ) =
  t t qB  k ,
The   R   function    ‘optim’    is   applied    to    Eq.   (40)    producing
 tsr
 2	t 	2	t 	2
  nu
  hMRlp'' 'hFil--hLwh
 
 
'''bbzvv-'
  o
 
 
 
  α
 
 CAD    lVh
-.h-l-
 
 
 
1r.
  or--1
3	4μ' haAHV ....A. ... ..:: . ...."  .................
 
 .	...  '  ...".. .
3γ1
 A1gorimsteps
 
--------mM
El n
 
 -=M
 
 
Iet) ±
 F( )+ 2μ(1q;	)+Wtur;
 A|	] =	[x  t(	)+3REl--(ey )
Gkt	I	1
 + L e--a	åt 	) "(-yL4-a-2
 )I  Æ4|
9î1t;ïq	9i1t;ïq
 =( 9 )Xk +9 ) 	+ q(9 )(μ9 ). 	(41)
Fig. 7. Evolution  of parameter estimates for   ,   , and    under a 3-state HOHMM. where Xk  denotes the actual value of the observation at time k; Xk stands for its corresponding prediction; X is the mean of all Xk ’s; and m = 1460, which is the total number of predicted values.
 adjusted p-values in our pairwise comparison were computed via the Bonferroni’s method using the R function ‘p.adjust’; this addresses the issue caused by familywise errors. The p-values for the comparison of 2state versus 3-state HOHMMs are large; so, we cannot reject the null hypothesis of no RMSE-mean differences at a significance level of 5%. This agrees with  Figs. 9 and 10, where regimes 2 and 3 behave similarly.  Thus, introducing  a third  state will  generate minimal  gain (if any). Error metrics for the 2-state and 3-state settings in Table 4 show very close results. However, comparison tests demonstrate that the 1state and 2-state settings’ error metric values are statistically different, and the same can be said concerning those for the 1-state and 3-state settings. Additionally,  Fig. 10 and Table 4 lend support to such an outcome, which clearly suggests the merit of incorporating regimeswitching features in the model for our data.
Fig. 8. Evolution  of parameter estimates for µ , and	under a 3-state HOHMM.
 B logL (Xk+ 1; ,   ,  , µ ,    ) =
 N yk , et
 ×   log (2   (   (yk ) +
 
   1 q   (yk ) )  2
2	w
 2    w	2     2    w
Akaike Information Criterion (AIC), to provide a measure that balances
 (X	(yw)    (yw )X    µ (yw ) q  (yw ) )2
 k      k	k	k
 2 ( 2 (yw) +
 ,
 k	k	k
 (43)
AIC =
 
 
 where B is the number of observed values and N is the number of states. The number of parameters to be estimated depends upon N, and this is
 summarised in Table 6. As expected, there is a significant increase in the number of parameters to be estimated  as the number of states grows
 Xk+1,  the  corresponding log-likelihood
 in a model.
Interval  of standard errors for parameter estimates under the 1-, 2-, 3-state HMMs and HOHMMs.
Parameter Estimates 	1-state model 	2-state model 	3-state model
Bound of SE
Lower 	Upper 	Lower 	Upper 	Lower 	Upper
Fig. 9. One-step ahead forecasts for Xk  and Sk  under a 3-state HOHMM.
 this  with  the  generation of  BIC values generated by  our  dynamic parameter estimation method. The BIC is computed as the best-model criterion  (cf [51])  when one is confronted by a collec-
 BIC =
 llogB + 2logL (X ; E ).
 (44)
The AICs in Table 7 come from a static estimation. We complement
 By setting B as the number of observations in each algorithm pass when employing Eq. (43), a series of BIC values are obtained as the data set is processed in its entirety.  Given the model choice-metric form in Eq. (44), the underlying principle of selection is to maximise the BIC function. Fig. 11 depicts the evolution of the calculated BIC values for the 1-, 2-, and 3-state models.
For the 1-state model, we get higher BIC values for almost all of the periods spanned by the algorithm passes in the whole data set. We note,
Fig. 10. Comparison of one-step ahead forecasts in 1-, 2-, 3-state HMMs and HOHMMs.
Error analysis of the HMMand HOHMM-based models for DSP. nonetheless, that  for  periods  where  jumps  occur,  the  BIC values markedly drop under the 1-state model. In contrast, BIC values produced by the regime-switching models have more stable patterns. Furthermore, the 2-state HOHMM even produces BIC values higher than those from  the 1-state HOHMM/HMM  for algorithm  steps that
 Table 6
Number of estimated parameters under HMM-OU-jump and HOHMM-OU-jump settings.
Model 	1-state 	2-state 	3-state 	N-state
 HMM 	5 	12 	21
HOHMM 	5 	14 	33
Comparison of selection criteria  AIC.
 N2 + 4N N3  N2 + 5N
 Model 	Number  of parameters 	AIC
1-state model 	5 	3005.51
3-state HOHMM 	33 	2897.54 w	w	w ging, which entails the pricing of these contracts. Forward/futures price
 G (t,  T ) =
 [ST |Xt] = D (T )exp  Xt e
 (yt ) (T   t) + (1  e
 (yt )(T   t) )  (yt )
 2 (yw) (1  e
 2  (yw) (T   t) )
 
T	(yw)(T  u) from market data (e.g., Fleten and Lemming [53]).  The second strand is to derive forward prices as the expected future spot prices at delivery
 + 	2  (yw)
 × 	exp 		e	t s
 dJu |Xt
2    w
 
2  (yw) (T   t) w  T   t
 w  T  t 	w
 (yt ) (1  e	t 	)
In this paper, we first identify  the best-fitting model for our DESP
 = D (T ) exp  Xt e
 (yt ) (      ) + (1  e
 (yt )(      ) )  (yt ) +
 2  (yw)
T	w	(yw)(T  u)	2    w
 2  (yw) (T   t) data. Then, optimal estimates using our filtering-based calibration are
 × exp      s     exp (µ  (yt ) e	t
 +     (yt ) e	t
 ) du
 (T  t )  .
 
(46) current time is t. With EFSP as the underlying variable, the theoretical forward price F (t, T ) of an electricity contract is computed as
 Derivation details of (46) are similar to those given in Benth et al. [54].  The optimal parameter estimates in the evaluation of the EFSP in
F (t,  T ) =
 (t,  T ) + G (t,  T ),	(45)
 Eq. (46) are shown in Table 8. It is reasonable to assume a constant
 
(t,  T ) has two contributing  parts: (i) the price of risk due to
 Markov chain when pricing EFSP over a short-term delivery period. For a longer maturity of T > 30 days, EFSP has to be numerically computed
 with  a dynamic Markov chain involved, as recommended in Erlwein
Bonferroni-corrected  p-values for the t-test performed on the RMSEs involving the DSP.
Fig. 11. Evolution  of BIC values under the 1-, 2-, 3-state HMMand HOHMM-based models. contracts with  short delivery horizon remains disputable. In our implementation, we simulate dynamic states through an HOHMC to obtain estimated values of the EFSP for a general delivery period. Fig. 12 shows the simulated EFSP with time t mapping the last 30 days of our dataset for maturities T = 1, …, 30 days. Salient features of the data are replicated by our proposed model such as the short-term fluctuations, seasonal patterns, and spikes of electricity prices. Our pricing application  is of practical significance, as the forward  contract valuation  is
 better fit than those achieved by the usual HMMs. The 2-state HOHMM outperforms other model settings for our data in accordance with  an information-criterion evaluation. immediate once
 (t,  T )’s estimate is available. Determining  the ap-
 modelling geared towards derivative valuation and risk measurement.
 (t,  T ) is beyond the scope of this article. But, we certainly
 A direct and natural direction of this work is a further empirical test
 of the modelling framework and estimation being put forward in the analysis of various contracts (more sophisticated than forwards) in the electricity market for investment and hedging. Applications in product development involving  latent factors moving in tandems, such as weather indices and electricity  prices, are of a particular  interest to practitioners.  For instance, a dynamic risk management strategy involving  electricity  and temperature-based futures could be supported by the HOHMM filtering  algorithms in finding  optimal  hedge ratios. Moreover, the proposed model could support dynamic revenue maximisation and management (Broll  and Wong [55]),  evaluation of policies in the electricity portfolio problem (Murgia and Sbrilli [56]),  and investment strategies in a basket of electricity derivatives if extended to a multivariate  regime-switching paradigm (Dong et al. [57]).
Optimal  parameter estimates for the 2-state HOHMM.
 Discovery Grant (No.: RGPIN/341780-2017-04235 ). H. Xiong acknowledges gratefully the Ontario Graduate Scholarship programme for the support of his doctoral research. R. Mamon expresses his sincere appreciation for the hospitality of the Division of Physical Sciences and Mathematics, University of the Philippines Visayas, where certain revisions of this paper were made during an academic visit both as an Adjunct  Professor and a DOST-PCIEERD  Balik  Scientist for  the Philippine government.
A.1. Optimal estimate for w	w
For k	0, define the Radon-Nikodym derivative of PE	with respect to PE
 
 w
 dPE
 
=    w   =
  k l=0
  w  , where
 
0 = 1, and
1	   	w      	w	   	w w l +1 =
 exp
 2 2 (yw ) (Xl+1
 (yl ) 	(yl  ) Xl
 µ (yl ) q
 (yl  ) )
1	   	w      	w	   	w w
 2 2 (yw ) (Xl +1
 (yl ) 	(yl  ) Xl
 µ (yl ) q
 (yl  ) )
2     w	2
  w	w   	 k 	(yl  ) (Xl
 + 2µ (yw) qXl + (µ (yw) q) )
 2  (yl  ) (Xl + µ (yw) q) (  (yl )
 Xl+1 ) w	 	l  	l  	l  		w log   k    = 	2    w
 2    w	+ R (  (yl ) ) l=1 k 	N
  yw , et
 2   (yl  )
 2   (yl  )
= 	 	l  	(  2 (Xl
 + 2µ qX + (µ q) ) + 2  (X + µ q) (
 X +  ) )
 + R (  )  , l=1
 2
 l 	l 	l   1	t where R ( t ) is a remainder free of   t . With Eqs. (21) and (23), we consider the expectation of the log likelihood,  i.e., L (  ) =
 
[log
  k  |Xk], where
[log
  k  |Xk] =
  k
 
  yw, et
2
 
(  2 (X 2
 
+ 2µ qXl + (µ q) ) + 2  (Xl + µ q) (
 
Xl+1) ) ) + R( t )
 
Xk   .
By differentiating  L (  ) with respect to 	and setting the result to 0, we get the optimal estimate   , which is t 	t 	t 	t
C  k (Xk , Xk+1)
 t C  k (Xk ) + µ t qC  k (Xk+1)
 t qµ  B  k t = 	t 	2	t 	2	.
C  k (Xk ) + 2µ t qC  k (Xk ) + (µ t q) B  k
A.2. Optimal estimate for w
Define a new measure PE	based on Eq. (15). Consider the construction dPE
 
=    w   = 	k
  w  , where	= 1, and
1	   	w      	w	   	w w
 dPE w	k
 l=0    l 	0 l +1 =
 exp
 2 2 (yw ) (Xl +1
 (yl ) 	(yl  ) Xl
 µ (yl ) q
 (yl  ) )
1	   	w      	w	   	w w
 2 2 (yw ) (Xl +1
 (yl ) 	(yl  ) Xl
 µ (yl ) q
 (yl  ) )
So, k 	2    w      	w l
  w	w	l 	w
  k 	N	w w	(yl )  2X +1
 (yl ) + 2  (yl  )  (yl  ) (X + µ (yl ) q)	w
 yl , et	2 log   k    =
 2    w	+ R (  (yl ) )   =
 2      ( 	2Xl+1
 + 2   (Xl + µ q) )
 + R ( t)  ,
 2   (yl  )
 l=1
 t=1 	2 t where R ( t) is a remainder that does not contain   t .
On the basis of Eqs. (21) and (23), the expectation of the log-likelihood  depending on Xk is L ( ) =
 
[log
  k   |Xk], where
[log
  k   |Xk] =
  k
 
  yw, et
2
 
2
( 	2Xl +1
 
+ 2   (Xl  + µ q) )
 
+ R ( t ) )
 
Xk   .
Solving  L (  )  and equating the result to 0, we get the optimal estimate t
C  k (Xk+1)
  t t C  k (Xk )
  t t µ  qB  k (Xk )
A.3. Optimal estimate for w
Define a new measure PE     , using Eq. (15), by setting dPE
 
=    w   = 	k 	w , where	= 1, and
1	w	w    l
 
(yw)	w   2
 dPE w	k
 l =0    l 	0 w	(Xl+
 (yl ) 	(yl  ) X     µ
 q  (y  ) )
(yl ) exp 	2   w	w
2(^ (y  ) +   2	q 2 (y  ) ) w	(yl ) l +1 =
 . w	w	(yw)	w   2
 (yl )      (yl  )Xl    µ
 q  (y  )) l
^ (yl ) exp
 2   w	2	2   w
2 (   (yl ) +
 (yw) q
 (yl ) ) k 	   	w      	w	   	w	w    2 w	      1
 (Xl   1 w
 (yl ) 	(yl  ) Xl
 µ (yl ) q
 (yl  ) ) log   k    =
 log
 w	+ R( (yl ) )
 2    w	2	2     w
 ^ (yl  )
 2(^ (yl ) +
 (yw) q
 (yl  ) ) k 	N	1	(X +1
 
(yw)	(yw ) Xl   µ
  w q  (yw ) )2 w	w	l 	l 	l
 (yl ) 	l
= 	yl , et
 log
 (yl )
 2    w	2	2     w
 + R ( t )  ,
 t=1
 ^ 	2 (^ (yl ) +
 (yw) q
 (yl  ) ) where R ( t) is independent of ^ . As per (21) and (23), we have k 	N w	w	 1   w	1
 
   	w      	w	   	w    2
[log
 k  |Xk] =
 yl  , et
 log
 (yl )
 2    w	2	2     w
 × (Xl+1
 (yl ) 	(yl  ) Xl
 µ (yw) q
 (yl  ) ) )
 Xk     + R ( t ).
 
 ^ 	2(^ (yl ) +
 (yw) q
 (yl  ) )
Equating to 0 the mathematical derivative (with  respect to ^) of L (^) =
 
[log
  k  |Xk], our optimal estimate of ^  is t 	2	2	t 	2
 t     2	2	     2    2	t 	2	t 	t
^2	C  k (Xk+1) +
 t  C  k (Xk ) + B k (  t  + ( t µ t p)
 + 2 t   t µ  q
 t    t  q)
 C  k (Xk ) (2µ t q t   + 2 t   t )   (2 t  + 2 t µ t p) C  k (Xk+1) + 2 t C  k (Xk+1 Xk ) t  = 	t 	+ 	t 	t 	.
A.4. Optimal estimate for µ w
Starting with Eq. (15), define a new measure PE	through dP
  wµ	k
  wµ
, where	= 1, and
1	   	w     	w	 	
 dPE w	k
Xk w    2
 l=1    l 	0 exp
 2   w  (Xl +
 (yl ) 	(yl  ) Xl   µ
 w q  (yl  ) ) wµ l +1   =
 2   (yl  ) 	1	(yl )
1	   	w      	w	   	w w
 2 2 (yw ) (Xl +1
 (yl ) 	(yl  ) Xl
 µ (yl ) q
 (yl  ) ) w      	w	w wµ	k 	(µ (yw) q
 (yl )  2Xl+1 + 2 (  (yl ) +
 (yl  ) ) )
 	l  		w log   k      =
 2     w	× (µ (yw) q
 (yl ) ) + R (µ (yw) ) k	N
  yw , et
 2   (yl  ) 	l 	l
 w q  (yw ) (µ
 w q  (yw)  2Xl +
 + 2(  (yw) +
 (yw) ) )
 + R (µ  ),
2	(y  )	l
 (y  ) 	l 	1	l 	l 	t l =1
 t=1
 2 t 	l 	l where R (µ
 
) does not contain µ
 
. Again, by (15), wµ	k 	N	yw, et
X	w	w
  w	w   	
  w	w	X
[log   k
 |  k] =
 l=1
 t=1
 2     µ (yl ) q
 (yl  ) (µ (yl ) q
 (yl )  2Xl+1 + 2 (  (yl ) +
 (yl  ) ) )
 R (µ  ). t
C  k (Xk+1)
  t qB  k (Xk )
 
A.5. Optimal estimate for w
From Eq. (15), define a new measure PE	by setting dPE
  w	k 	w
 
, where	= 1, and w	w	w   2
 dPE w	k
 l=1    l 	0
(yw) exp
 (Xl+ 1
 (yl )      (yl  )Xl w
 µ  (yw) q  (yl  )) w
 2 ( 2 (yl ) + w
 2   w q 2 (yl ) w
 (yw ))
. w
(yw) exp
 (Xl+ 1
 (yl )      (yl )Xl
 µ  (yw) q  (yl ))2 l 	2 ( 2 (yl ) +
 2   w      2 (yw) ) l
This yields the log likelihood k
 
   	w      	w	   	w	w    2 w	1	(Xl+1
 (yl ) 	(yl  ) Xl
 µ (yl ) q
 (yl  ) ) log   k     =
 log
 2    w	2	2     w
 + R (
 (yl ))
 2 (   (yl ) +
 (yw) q
 (yl  ) )
   	w      	w	   	w    2 k 	N	1	(Xl+1
 (yl ) 	(yl  ) Xl
 µ (yw) q
 (yl  ) )
 log
 2    w	2	2     w
 + R (
 t ),
 2 (   (yl ) +
 (yw) q
 (yl  ) ) where R ( t) does not depend on ^ . Invoking Eq. (15), k 	N w	w	1	1
 
   	w      	w	   	w    2
 |Xk] =
 yl  , et
 log
 2    w	2	2     w
 × (Xl +1
 (yl ) 	(yl  ) Xl
 µ (y  ) q
 (yl  ) ) )
 Xk    + R (   )
 t=1 	(yl )
 2 (   (yl ) +
 (yw) q
 w	t
(yl  ) )
N
 
 
(Xl +1       	
 
Xl     µ q )2
 
+ R (
 
 2(    + 	q   )
The optimal estimate for   2 is t 	2	2	t 	2
 t     2	2	t
 
    2	t 	2
2	C  k (Xk+1) +
 t  C  k (Xk ) + B k (  t  + ( t µ t q) )
 B k (2 t   t µ t q
 t ) + C  k (Xk ) (2µ t q t   + 2 t   t ) t  = 	t 	+
B k   t  q t 	t
 t
B k   t  q
(2 t  + 2 t µ  q) C  k (Xk+1) + 2 t C  k (Xk+1 Xk ) t 	.
B k   t  q w
To define a new measure PE	described in Eq. (32), consider the Radon-Nikodym derivative  dPE
 
=    p , where w	w	w
 dPE w	k k 	N p	tsr
 yl   2,er   yl   1, es    yl ,et k  =
 
 . k 	N	yw  ,er
  yw  ,es    yw,et	k 	N p	ptsr      l   2
 l   1	l
    	w	w	w
[log
 k |Xk] =
 
 log
 ptsr
 Xk     =
 
 (logptsr
 logptsr ) yl   2, er
 yl   1, es
 yl , et	Xk
N
  logptsr A tsr
 
Xk     + R (ptsr ),
L(p) =	L  logpt.r. F + pl L ptsr-1  1 +	) ø1 -tsr + P = 0
 
 [2      
[3]   
 
 
 [30]   
[43]  
 [μ4η r
 
 r
 [58]   
